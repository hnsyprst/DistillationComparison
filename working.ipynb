{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.profiler.emit_nvtx at 0x24865e9d4b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "\n",
    "import training_utils as utils\n",
    "import distillation_methods_module\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)\n",
    "\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "torch.autograd.profiler.profile(False)\n",
    "torch.autograd.profiler.emit_nvtx(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the training and test sets\n",
    "trans = transforms.ToTensor()\n",
    "mnist_train = torchvision.datasets.FashionMNIST(\n",
    "    root=\"../data\", train=True, transform=trans, download=True)\n",
    "mnist_test = torchvision.datasets.FashionMNIST(\n",
    "    root=\"../data\", train=False, transform=trans, download=True)\n",
    "\n",
    "train_iter = data.DataLoader(mnist_train, batch_size=256, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_iter = data.DataLoader(mnist_test, batch_size=256, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Teacher_Net(nn.Module):\n",
    "    def __init__(self, dropout=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear_1 = nn.Linear(784, 1200)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.linear_2 = nn.Linear(1200, 1200)\n",
    "\n",
    "        self.linear_3 = nn.Linear(1200, 1200)\n",
    "\n",
    "        self.linear_4 = nn.Linear(1200, 10)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input = torch.flatten(input, start_dim=1)\n",
    "        out = self.linear_1(input)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.linear_2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.linear_3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.linear_4(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Student_Net(nn.Module):\n",
    "    def __init__(self, dropout=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear_1 = nn.Linear(784, 600)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.linear_2 = nn.Linear(600, 1200)\n",
    "\n",
    "        self.linear_3 = nn.Linear(1200, 1200)\n",
    "\n",
    "        self.linear_4 = nn.Linear(1200, 10)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input = torch.flatten(input, start_dim=1)\n",
    "        out = self.linear_1(input)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.linear_2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.linear_3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.linear_4(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:\t\t  0\n",
      "train_metrics:\t  (0.6198176907221477, 0.7714666666666666)\n",
      "test_accuracy:\t  0.8236\n",
      "Epoch:\t\t  1\n",
      "train_metrics:\t  (0.44270379422505696, 0.8398333333333333)\n",
      "test_accuracy:\t  0.854\n",
      "Epoch:\t\t  2\n",
      "train_metrics:\t  (0.40867470410664875, 0.85095)\n",
      "test_accuracy:\t  0.8562\n",
      "Epoch:\t\t  3\n",
      "train_metrics:\t  (0.3908185957590739, 0.8564333333333334)\n",
      "test_accuracy:\t  0.8641\n",
      "Epoch:\t\t  4\n",
      "train_metrics:\t  (0.3861622578620911, 0.8586333333333334)\n",
      "test_accuracy:\t  0.8673\n",
      "Epoch:\t\t  5\n",
      "train_metrics:\t  (0.37064978942871096, 0.8637666666666667)\n",
      "test_accuracy:\t  0.8739\n",
      "Epoch:\t\t  6\n",
      "train_metrics:\t  (0.3611437030792236, 0.8684166666666666)\n",
      "test_accuracy:\t  0.8724\n",
      "Epoch:\t\t  7\n",
      "train_metrics:\t  (0.35373455613454186, 0.8693)\n",
      "test_accuracy:\t  0.879\n",
      "Epoch:\t\t  8\n",
      "train_metrics:\t  (0.34425810718536376, 0.8747666666666667)\n",
      "test_accuracy:\t  0.8749\n",
      "Epoch:\t\t  9\n",
      "train_metrics:\t  (0.3427658058802287, 0.8749833333333333)\n",
      "test_accuracy:\t  0.8731\n"
     ]
    }
   ],
   "source": [
    "teacher = Teacher_Net().to(device)\n",
    "num_epochs = 10\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='none').to(device)\n",
    "optimizer = torch.optim.Adam(teacher.parameters(), lr=0.001)\n",
    "history = utils.train(teacher, utils.train_epoch, train_iter, test_iter, loss_fn, num_epochs, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:\t\t  0\n",
      "train_metrics:\t  (1.896868804275679e-05, 0.7019666666666666)\n",
      "test_accuracy:\t  0.801\n",
      "Epoch:\t\t  1\n",
      "train_metrics:\t  (8.331771542240555e-06, 0.7899333333333334)\n",
      "test_accuracy:\t  0.8199\n",
      "Epoch:\t\t  2\n",
      "train_metrics:\t  (7.88387215191809e-06, 0.7968166666666666)\n",
      "test_accuracy:\t  0.8185\n",
      "Epoch:\t\t  3\n",
      "train_metrics:\t  (8.198371655695762e-06, 0.7983666666666667)\n",
      "test_accuracy:\t  0.8249\n",
      "Epoch:\t\t  4\n",
      "train_metrics:\t  (6.990285400145997e-06, 0.8082166666666667)\n",
      "test_accuracy:\t  0.8288\n",
      "Epoch:\t\t  5\n",
      "train_metrics:\t  (7.5786689568000535e-06, 0.8020666666666667)\n",
      "test_accuracy:\t  0.824\n",
      "Epoch:\t\t  6\n",
      "train_metrics:\t  (7.848203416991357e-06, 0.7991166666666667)\n",
      "test_accuracy:\t  0.8279\n",
      "Epoch:\t\t  7\n",
      "train_metrics:\t  (7.536703136671955e-06, 0.8046333333333333)\n",
      "test_accuracy:\t  0.8321\n",
      "Epoch:\t\t  8\n",
      "train_metrics:\t  (8.323327700297038e-06, 0.7936333333333333)\n",
      "test_accuracy:\t  0.8292\n",
      "Epoch:\t\t  9\n",
      "train_metrics:\t  (8.293020620476455e-06, 0.7955666666666666)\n",
      "test_accuracy:\t  0.8258\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.7019666666666666,\n",
       "  0.7899333333333334,\n",
       "  0.7968166666666666,\n",
       "  0.7983666666666667,\n",
       "  0.8082166666666667,\n",
       "  0.8020666666666667,\n",
       "  0.7991166666666667,\n",
       "  0.8046333333333333,\n",
       "  0.7936333333333333,\n",
       "  0.7955666666666666],\n",
       " [1.896868804275679e-05,\n",
       "  8.331771542240555e-06,\n",
       "  7.88387215191809e-06,\n",
       "  8.198371655695762e-06,\n",
       "  6.990285400145997e-06,\n",
       "  7.5786689568000535e-06,\n",
       "  7.848203416991357e-06,\n",
       "  7.536703136671955e-06,\n",
       "  8.323327700297038e-06,\n",
       "  8.293020620476455e-06],\n",
       " [0.801,\n",
       "  0.8199,\n",
       "  0.8185,\n",
       "  0.8249,\n",
       "  0.8288,\n",
       "  0.824,\n",
       "  0.8279,\n",
       "  0.8321,\n",
       "  0.8292,\n",
       "  0.8258])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student = Student_Net().to(device)\n",
    "optimizer = torch.optim.Adam(student.parameters(), lr=5e-3)\n",
    "\n",
    "d_logits = distillation_methods_module.Logits_Distiller(5, teacher=teacher, student=student, optimizer=optimizer)\n",
    "d_logits.train(train_iter, test_iter, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (7168x28 and 784x600)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [14], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(student\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[0;32m      4\u001b[0m d_features \u001b[38;5;241m=\u001b[39m distillation_methods_module\u001b[38;5;241m.\u001b[39mFeatures_Distiller(hint_layer\u001b[38;5;241m=\u001b[39mteacher\u001b[38;5;241m.\u001b[39mlinear_2, guided_layer\u001b[38;5;241m=\u001b[39mstudent\u001b[38;5;241m.\u001b[39mlinear_2, teacher\u001b[38;5;241m=\u001b[39mteacher, student\u001b[38;5;241m=\u001b[39mstudent, optimizer\u001b[38;5;241m=\u001b[39moptimizer)\n\u001b[1;32m----> 5\u001b[0m \u001b[43md_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Work\\UNI\\QMUL\\Project\\repo\\DistillationComparison\\distillation_methods_module\\features_distiller.py:117\u001b[0m, in \u001b[0;36mFeatures_Distiller.train\u001b[1;34m(self, train_set, test_set, num_epochs)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m, train_set, test_set, num_epochs): \n\u001b[1;32m--> 117\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_stage_1(train_set, test_set, num_epochs)\n\u001b[0;32m    118\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_stage_2(train_set, test_set, num_epochs)\n",
      "File \u001b[1;32md:\\Work\\UNI\\QMUL\\Project\\repo\\DistillationComparison\\distillation_methods_module\\features_distiller.py:56\u001b[0m, in \u001b[0;36mFeatures_Distiller.train_stage_1\u001b[1;34m(self, train_set, test_set, num_epochs)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhint_layer\u001b[39m.\u001b[39mregister_forward_hook(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_feature_map(\u001b[39m'\u001b[39m\u001b[39mhint_layer\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m     55\u001b[0m loss_fn \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mMSELoss()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> 56\u001b[0m \u001b[39mreturn\u001b[39;00m utils\u001b[39m.\u001b[39;49mtrain(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprehint_student, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_epoch_stage_1, train_set, test_set, loss_fn, num_epochs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer)\n",
      "File \u001b[1;32md:\\Work\\UNI\\QMUL\\Project\\repo\\DistillationComparison\\training_utils.py:77\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(net, train_epoch_fn, train_iter, test_iter, loss_fn, num_epochs, optimizer)\u001b[0m\n\u001b[0;32m     74\u001b[0m history_test_accuracy \u001b[39m=\u001b[39m []\n\u001b[0;32m     76\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 77\u001b[0m     train_metrics \u001b[39m=\u001b[39m train_epoch_fn(net, train_iter, loss_fn, optimizer)\n\u001b[0;32m     78\u001b[0m     test_acc \u001b[39m=\u001b[39m evaluate_accuracy(net, test_iter)\n\u001b[0;32m     80\u001b[0m     \u001b[39m# Log the model history\u001b[39;00m\n",
      "File \u001b[1;32md:\\Work\\UNI\\QMUL\\Project\\repo\\DistillationComparison\\distillation_methods_module\\features_distiller.py:68\u001b[0m, in \u001b[0;36mFeatures_Distiller.train_epoch_stage_1\u001b[1;34m(self, net, train_set, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m     65\u001b[0m features \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     66\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> 68\u001b[0m preds \u001b[39m=\u001b[39m net(features)\n\u001b[0;32m     69\u001b[0m teacher_preds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mteacher(features)\n\u001b[0;32m     71\u001b[0m teacher_hints \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_map[\u001b[39m'\u001b[39m\u001b[39mhint_layer\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32md:\\Work\\UNI\\QMUL\\Project\\repo\\DistillationComparison\\.distenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Work\\UNI\\QMUL\\Project\\repo\\DistillationComparison\\.distenv\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32md:\\Work\\UNI\\QMUL\\Project\\repo\\DistillationComparison\\.distenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Work\\UNI\\QMUL\\Project\\repo\\DistillationComparison\\.distenv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (7168x28 and 784x600)"
     ]
    }
   ],
   "source": [
    "student = Student_Net().to(device)\n",
    "optimizer = torch.optim.Adam(student.parameters(), lr=1e-3)\n",
    "\n",
    "d_features = distillation_methods_module.Features_Distiller(hint_layer=teacher.linear_2, guided_layer=student.linear_2, teacher=teacher, student=student, optimizer=optimizer)\n",
    "d_features.train(train_iter, test_iter, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:\t\t  0\n",
      "train_metrics:\t  (218.26622141927083, 0.10048333333333333)\n",
      "test_accuracy:\t  0.0989\n",
      "Epoch:\t\t  1\n",
      "train_metrics:\t  (84.02734996744792, 0.10025)\n",
      "test_accuracy:\t  0.1\n",
      "Epoch:\t\t  2\n",
      "train_metrics:\t  (66.88911232096355, 0.10098333333333333)\n",
      "test_accuracy:\t  0.1\n",
      "Epoch:\t\t  3\n",
      "train_metrics:\t  (59.55752234700521, 0.10118333333333333)\n",
      "test_accuracy:\t  0.1\n",
      "Epoch:\t\t  4\n",
      "train_metrics:\t  (54.03042255859375, 0.10051666666666667)\n",
      "test_accuracy:\t  0.1\n",
      "Epoch:\t\t  5\n",
      "train_metrics:\t  (60.741406640625, 0.09818333333333333)\n",
      "test_accuracy:\t  0.1\n",
      "Epoch:\t\t  6\n",
      "train_metrics:\t  (50.79052355957031, 0.10016666666666667)\n",
      "test_accuracy:\t  0.1\n",
      "Epoch:\t\t  7\n",
      "train_metrics:\t  (47.62262318522136, 0.10048333333333333)\n",
      "test_accuracy:\t  0.1\n",
      "Epoch:\t\t  8\n",
      "train_metrics:\t  (46.51358276367188, 0.10021666666666666)\n",
      "test_accuracy:\t  0.1\n",
      "Epoch:\t\t  9\n",
      "train_metrics:\t  (46.01889170735677, 0.09996666666666666)\n",
      "test_accuracy:\t  0.1\n",
      "Epoch:\t\t  0\n",
      "train_metrics:\t  (1.44498934173584, 0.5841833333333334)\n",
      "test_accuracy:\t  0.7934\n",
      "Epoch:\t\t  1\n",
      "train_metrics:\t  (0.9893265190124512, 0.7289166666666667)\n",
      "test_accuracy:\t  0.8137\n",
      "Epoch:\t\t  2\n",
      "train_metrics:\t  (0.8613761878967285, 0.7474166666666666)\n",
      "test_accuracy:\t  0.8229\n",
      "Epoch:\t\t  3\n",
      "train_metrics:\t  (0.7948878176371257, 0.7585166666666666)\n",
      "test_accuracy:\t  0.8242\n",
      "Epoch:\t\t  4\n",
      "train_metrics:\t  (0.7473975632985433, 0.7671666666666667)\n",
      "test_accuracy:\t  0.8248\n",
      "Epoch:\t\t  5\n",
      "train_metrics:\t  (0.712172764968872, 0.7769666666666667)\n",
      "test_accuracy:\t  0.8323\n",
      "Epoch:\t\t  6\n",
      "train_metrics:\t  (0.6828827878316244, 0.7821666666666667)\n",
      "test_accuracy:\t  0.8347\n",
      "Epoch:\t\t  7\n",
      "train_metrics:\t  (0.6606242468516031, 0.7873666666666667)\n",
      "test_accuracy:\t  0.8311\n",
      "Epoch:\t\t  8\n",
      "train_metrics:\t  (0.6374113406499227, 0.7937)\n",
      "test_accuracy:\t  0.8348\n",
      "Epoch:\t\t  9\n",
      "train_metrics:\t  (0.6198916083017985, 0.7957166666666666)\n",
      "test_accuracy:\t  0.8381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.5841833333333334,\n",
       "  0.7289166666666667,\n",
       "  0.7474166666666666,\n",
       "  0.7585166666666666,\n",
       "  0.7671666666666667,\n",
       "  0.7769666666666667,\n",
       "  0.7821666666666667,\n",
       "  0.7873666666666667,\n",
       "  0.7937,\n",
       "  0.7957166666666666],\n",
       " [1.44498934173584,\n",
       "  0.9893265190124512,\n",
       "  0.8613761878967285,\n",
       "  0.7948878176371257,\n",
       "  0.7473975632985433,\n",
       "  0.712172764968872,\n",
       "  0.6828827878316244,\n",
       "  0.6606242468516031,\n",
       "  0.6374113406499227,\n",
       "  0.6198916083017985],\n",
       " [0.7934,\n",
       "  0.8137,\n",
       "  0.8229,\n",
       "  0.8242,\n",
       "  0.8248,\n",
       "  0.8323,\n",
       "  0.8347,\n",
       "  0.8311,\n",
       "  0.8348,\n",
       "  0.8381])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student = Student_Net().to(device)\n",
    "optimizer = torch.optim.Adam(student.parameters(), lr=1e-3)\n",
    "\n",
    "hint_layers = (teacher.linear_2, teacher.linear_3)\n",
    "guided_layers = (student.linear_2, student.linear_3)\n",
    "\n",
    "d_relation = distillation_methods_module.Relations_Distiller(hint_layers=hint_layers, guided_layers=guided_layers, teacher=teacher, student=student, optimizer=optimizer)\n",
    "d_relation.train(train_iter, test_iter, num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.distenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d0557164c183bc22d7390206f2e8a2deecaeb7985853eddae99ef9c3a8b0152d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
