{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.profiler.emit_nvtx at 0x1aed1f6fa00>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "\n",
    "import training_utils as utils\n",
    "import network_utils as nutils\n",
    "\n",
    "import distillation_methods_module\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)\n",
    "\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "torch.autograd.profiler.profile(False)\n",
    "torch.autograd.profiler.emit_nvtx(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Download the training and test sets\n",
    "trans = transforms.ToTensor()\n",
    "mnist_train = torchvision.datasets.CIFAR10(\n",
    "    root=\"../data\", train=True, transform=trans, download=True)\n",
    "mnist_test = torchvision.datasets.CIFAR10(\n",
    "    root=\"../data\", train=False, transform=trans, download=True)\n",
    "\n",
    "train_iter = data.DataLoader(mnist_train, batch_size=256, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_iter = data.DataLoader(mnist_test, batch_size=256, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Teacher_Net(nn.Module):\n",
    "    def __init__(self, dropout=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear_1 = nn.Linear(784, 1200)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.linear_2 = nn.Linear(1200, 1200)\n",
    "\n",
    "        self.linear_3 = nn.Linear(1200, 1200)\n",
    "\n",
    "        self.linear_4 = nn.Linear(1200, 10)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input = torch.flatten(input, start_dim=1)\n",
    "        out = self.linear_1(input)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.linear_2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.linear_3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.linear_4(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Student_Net(nn.Module):\n",
    "    def __init__(self, dropout=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear_1 = nn.Linear(784, 600)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.linear_2 = nn.Linear(600, 1200)\n",
    "\n",
    "        self.linear_3 = nn.Linear(1200, 1200)\n",
    "\n",
    "        self.linear_4 = nn.Linear(1200, 10)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        input = torch.flatten(input, start_dim=1)\n",
    "        out = self.linear_1(input)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.linear_2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.linear_3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.linear_4(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Teacher_Net(nn.Module):\n",
    "    def __init__(self, dropout=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear_1 = nn.Conv2d(3, 32, (3,3))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.maxpool_1 = nn.MaxPool2d((2,2))\n",
    "\n",
    "        self.linear_2 = nn.Conv2d(32, 64, (3,3))\n",
    "        self.maxpool_2 = nn.MaxPool2d((2,2))\n",
    "\n",
    "        self.linear_3 = nn.Conv2d(64, 64, (3,3))\n",
    "        self.maxpool_3 = nn.MaxPool2d((2,2))\n",
    "\n",
    "        self.linear_4 = nn.Linear(256, 10)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        out = self.linear_1(input)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.maxpool_1(out)\n",
    "\n",
    "        out = self.linear_2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.maxpool_2(out)\n",
    "\n",
    "        out = self.linear_3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.maxpool_3(out)\n",
    "\n",
    "        \n",
    "        out = torch.flatten(out, start_dim=1)\n",
    "        out = self.linear_4(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Student_Net(nn.Module):\n",
    "    def __init__(self, dropout=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear_1 = nn.Conv2d(3, 32, (3,3))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.maxpool_1 = nn.MaxPool2d((2,2))\n",
    "\n",
    "        self.linear_2 = nn.Conv2d(32, 64, (3,3))\n",
    "        self.maxpool_2 = nn.MaxPool2d((2,2))\n",
    "\n",
    "        self.linear_3 = nn.Conv2d(64, 64, (3,3))\n",
    "        self.maxpool_3 = nn.MaxPool2d((2,2))\n",
    "\n",
    "        self.linear_4 = nn.Linear(256, 10)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        out = self.linear_1(input)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.maxpool_1(out)\n",
    "\n",
    "        out = self.linear_2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.maxpool_2(out)\n",
    "\n",
    "        out = self.linear_3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.maxpool_3(out)\n",
    "        \n",
    "        out = torch.flatten(out, start_dim=1)\n",
    "        out = self.linear_4(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Work\\UNI\\QMUL\\Project\\repo\\DistillationComparison\\.distenv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\Work\\UNI\\QMUL\\Project\\repo\\DistillationComparison\\.distenv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "PATH = r\"D:\\Work\\UNI\\QMUL\\Project\\Models\\ResNet152\\CIFAR100_e250_a6172.tar\"\n",
    "num_classes = 100\n",
    "\n",
    "teacher = models.resnet152(pretrained=False)\n",
    "num_ftrs = teacher.fc.in_features\n",
    "teacher.fc = nn.Linear(num_ftrs, num_classes)\n",
    "optimizer = torch.optim.Adam(teacher.parameters(), lr=0.001)\n",
    "\n",
    "checkpoint = torch.load(PATH)\n",
    "teacher.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "teacher.to(device)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "student = models.resnet101(pretrained=False)\n",
    "num_ftrs = student.fc.in_features\n",
    "student.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "student.to(device)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher = Teacher_Net().to(device)\n",
    "num_epochs = 10\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='none').to(device)\n",
    "optimizer = torch.optim.Adam(teacher.parameters(), lr=0.001)\n",
    "history = utils.train(teacher, utils.train_epoch, train_iter, test_iter, loss_fn, num_epochs, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student = Student_Net().to(device)\n",
    "optimizer = torch.optim.Adam(student.parameters(), lr=5e-3)\n",
    "\n",
    "d_logits = distillation_methods_module.Logits_Distiller(5, teacher=teacher, student=student, optimizer=optimizer)\n",
    "d_logits.train(train_iter, test_iter, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:\t\t  0\n",
      "train_metrics:\t  (5.326911418815143e-06, 0.0)\n",
      "test_accuracy:\t  0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [23], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m guided_layer \u001b[38;5;241m=\u001b[39m nutils\u001b[38;5;241m.\u001b[39mget_layer_in_module_from_index(student\u001b[38;5;241m.\u001b[39mlayer2[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m      9\u001b[0m d_features \u001b[38;5;241m=\u001b[39m distillation_methods_module\u001b[38;5;241m.\u001b[39mFeatures_Distiller_2(hint_layer\u001b[38;5;241m=\u001b[39mhint_layer, guided_layer\u001b[38;5;241m=\u001b[39mguided_layer, is_2D\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, teacher\u001b[38;5;241m=\u001b[39mteacher, student\u001b[38;5;241m=\u001b[39mstudent, optimizer\u001b[38;5;241m=\u001b[39moptimizer)\n\u001b[1;32m---> 10\u001b[0m \u001b[43md_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Work\\UNI\\QMUL\\Project\\repo\\DistillationComparison\\distillation_methods_module\\features_distiller_2.py:189\u001b[0m, in \u001b[0;36mFeatures_Distiller_2.train\u001b[1;34m(self, train_set, test_set, num_epochs)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m, train_set, test_set, num_epochs): \n\u001b[1;32m--> 189\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_stage_1(train_set, test_set, num_epochs)\n\u001b[0;32m    190\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_stage_2(train_set, test_set, num_epochs)\n",
      "File \u001b[1;32md:\\Work\\UNI\\QMUL\\Project\\repo\\DistillationComparison\\distillation_methods_module\\features_distiller_2.py:136\u001b[0m, in \u001b[0;36mFeatures_Distiller_2.train_stage_1\u001b[1;34m(self, train_set, test_set, num_epochs)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[39m# Perform the first stage of model training, using 'train_epoch_stage_1' fn to train the model each epoch\u001b[39;00m\n\u001b[0;32m    135\u001b[0m loss_fn \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mMSELoss()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m--> 136\u001b[0m \u001b[39mreturn\u001b[39;00m utils\u001b[39m.\u001b[39;49mtrain(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstage_1_student, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_epoch_stage_1, train_set, test_set, loss_fn, num_epochs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer)\n",
      "File \u001b[1;32md:\\Work\\UNI\\QMUL\\Project\\repo\\DistillationComparison\\training_utils.py:78\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(net, train_epoch_fn, train_iter, test_iter, loss_fn, num_epochs, optimizer)\u001b[0m\n\u001b[0;32m     75\u001b[0m history_test_accuracy \u001b[39m=\u001b[39m []\n\u001b[0;32m     77\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 78\u001b[0m     train_metrics \u001b[39m=\u001b[39m train_epoch_fn(net, train_iter, loss_fn, optimizer)\n\u001b[0;32m     79\u001b[0m     test_acc \u001b[39m=\u001b[39m evaluate_accuracy(net, test_iter)\n\u001b[0;32m     81\u001b[0m     \u001b[39m# Log the model history\u001b[39;00m\n",
      "File \u001b[1;32md:\\Work\\UNI\\QMUL\\Project\\repo\\DistillationComparison\\distillation_methods_module\\features_distiller_2.py:67\u001b[0m, in \u001b[0;36mFeatures_Distiller_2.train_epoch_stage_1\u001b[1;34m(self, net, train_set, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m     64\u001b[0m metric \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mAccumulator(\u001b[39m3\u001b[39m)\n\u001b[0;32m     66\u001b[0m \u001b[39m# Iterate over the current batch\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m \u001b[39mfor\u001b[39;00m features, labels \u001b[39min\u001b[39;00m train_set:\n\u001b[0;32m     68\u001b[0m     features \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     69\u001b[0m     labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32md:\\Work\\UNI\\QMUL\\Project\\repo\\DistillationComparison\\.distenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:444\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator\n\u001b[0;32m    443\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 444\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_iterator()\n",
      "File \u001b[1;32md:\\Work\\UNI\\QMUL\\Project\\repo\\DistillationComparison\\.distenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:390\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    389\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 390\u001b[0m     \u001b[39mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[1;32md:\\Work\\UNI\\QMUL\\Project\\repo\\DistillationComparison\\.distenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1077\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1070\u001b[0m w\u001b[39m.\u001b[39mdaemon \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1071\u001b[0m \u001b[39m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m \u001b[39m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1073\u001b[0m \u001b[39m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1074\u001b[0m \u001b[39m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1075\u001b[0m \u001b[39m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1076\u001b[0m \u001b[39m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1077\u001b[0m w\u001b[39m.\u001b[39;49mstart()\n\u001b[0;32m   1078\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_queues\u001b[39m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1079\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workers\u001b[39m.\u001b[39mappend(w)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m _current_process\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdaemon\u001b[39m\u001b[39m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdaemonic processes are not allowed to have children\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_Popen(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sentinel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen\u001b[39m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[39m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_context\u001b[39m.\u001b[39;49mget_context()\u001b[39m.\u001b[39;49mProcess\u001b[39m.\u001b[39;49m_Popen(process_obj)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\context.py:336\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    334\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    335\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpopen_spawn_win32\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n\u001b[1;32m--> 336\u001b[0m     \u001b[39mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     reduction\u001b[39m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 93\u001b[0m     reduction\u001b[39m.\u001b[39;49mdump(process_obj, to_child)\n\u001b[0;32m     94\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     set_spawning_popen(\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdump\u001b[39m(obj, file, protocol\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m     \u001b[39m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     ForkingPickler(file, protocol)\u001b[39m.\u001b[39;49mdump(obj)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "#student = Student_Net().to(device)\n",
    "optimizer = torch.optim.Adam(student.parameters(), lr=1e-3)\n",
    "\n",
    "hint_layer = nutils.get_layer_in_module_from_index(teacher.layer2[-1], -3)\n",
    "guided_layer = nutils.get_layer_in_module_from_index(student.layer2[-1], -3)\n",
    "\n",
    "d_features = distillation_methods_module.Features_Distiller(hint_layer=hint_layer, guided_layer=guided_layer, is_2D=True, teacher=teacher, student=student, optimizer=optimizer)\n",
    "d_features.train(train_iter, test_iter, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (256x16384 and 8192x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [11], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m guided_layers \u001b[38;5;241m=\u001b[39m (student\u001b[38;5;241m.\u001b[39mlayer1, student\u001b[38;5;241m.\u001b[39mlayer2)\n\u001b[0;32m      9\u001b[0m d_relation \u001b[38;5;241m=\u001b[39m distillation_methods_module\u001b[38;5;241m.\u001b[39mRelations_Distiller(hint_layers\u001b[38;5;241m=\u001b[39mhint_layers, guided_layers\u001b[38;5;241m=\u001b[39mguided_layers, teacher\u001b[38;5;241m=\u001b[39mteacher, student\u001b[38;5;241m=\u001b[39mstudent, optimizer\u001b[38;5;241m=\u001b[39moptimizer)\n\u001b[1;32m---> 10\u001b[0m \u001b[43md_relation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Work\\UNI\\QMUL\\Project\\repo\\DistillationComparison\\distillation_methods_module\\relations_distiller.py:139\u001b[0m, in \u001b[0;36mRelations_Distiller.train\u001b[1;34m(self, train_set, test_set, num_epochs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m, train_set, test_set, num_epochs):\n\u001b[1;32m--> 139\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_stage_1(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstudent, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mteacher, train_set, test_set, num_epochs)\n\u001b[0;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_stage_2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstudent, train_set, test_set, num_epochs)\n",
      "File \u001b[1;32md:\\Work\\UNI\\QMUL\\Project\\repo\\DistillationComparison\\distillation_methods_module\\relations_distiller.py:113\u001b[0m, in \u001b[0;36mRelations_Distiller.train_stage_1\u001b[1;34m(self, student_net, teacher_net, train_set, test_set, num_epochs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[39m# Perform the first stage of model training, using 'train_epoch_stage_1' fn to train the model each epoch\u001b[39;00m\n\u001b[0;32m    112\u001b[0m loss_fn \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mMSELoss()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m--> 113\u001b[0m \u001b[39mreturn\u001b[39;00m utils\u001b[39m.\u001b[39;49mtrain(student_net, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_epoch_stage_1, train_set, test_set, loss_fn, num_epochs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer)\n",
      "File \u001b[1;32md:\\Work\\UNI\\QMUL\\Project\\repo\\DistillationComparison\\training_utils.py:78\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(net, train_epoch_fn, train_iter, test_iter, loss_fn, num_epochs, optimizer)\u001b[0m\n\u001b[0;32m     75\u001b[0m history_test_accuracy \u001b[39m=\u001b[39m []\n\u001b[0;32m     77\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 78\u001b[0m     train_metrics \u001b[39m=\u001b[39m train_epoch_fn(net, train_iter, loss_fn, optimizer)\n\u001b[0;32m     79\u001b[0m     test_acc \u001b[39m=\u001b[39m evaluate_accuracy(net, test_iter)\n\u001b[0;32m     81\u001b[0m     \u001b[39m# Log the model history\u001b[39;00m\n",
      "File \u001b[1;32md:\\Work\\UNI\\QMUL\\Project\\repo\\DistillationComparison\\distillation_methods_module\\relations_distiller.py:76\u001b[0m, in \u001b[0;36mRelations_Distiller.train_epoch_stage_1\u001b[1;34m(self, net, train_set, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m     74\u001b[0m hint_start_layer_feature_map \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_map[\u001b[39m'\u001b[39m\u001b[39mhint_start_layer\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     75\u001b[0m hint_end_layer_feature_map \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_map[\u001b[39m'\u001b[39m\u001b[39mhint_end_layer\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m---> 76\u001b[0m teacher_FSPs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcalculate_FSP_matrix(hint_start_layer_feature_map, hint_end_layer_feature_map)\n\u001b[0;32m     78\u001b[0m \u001b[39m# The feature maps generated by the student model for this predicton are extracted\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[39m# and the FSP is calculated between them\u001b[39;00m\n\u001b[0;32m     80\u001b[0m guided_start_layer_feature_map \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_map[\u001b[39m'\u001b[39m\u001b[39mguided_start_layer\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32md:\\Work\\UNI\\QMUL\\Project\\repo\\DistillationComparison\\distillation_methods_module\\relations_distiller.py:45\u001b[0m, in \u001b[0;36mRelations_Distiller.calculate_FSP_matrix\u001b[1;34m(self, feature_map_1, feature_map_2)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcalculate_FSP_matrix\u001b[39m(\u001b[39mself\u001b[39m, feature_map_1, feature_map_2):\n\u001b[1;32m---> 45\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mmatmul(torch\u001b[39m.\u001b[39;49mflatten(feature_map_1, start_dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m), torch\u001b[39m.\u001b[39;49mflatten(feature_map_2, start_dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mT)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (256x16384 and 8192x256)"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "#student = Student_Net().to(device)\n",
    "optimizer = torch.optim.Adam(student.parameters(), lr=1e-3)\n",
    "\n",
    "hint_layers = (teacher.layer1, teacher.layer2)\n",
    "guided_layers = (student.layer1, student.layer2)\n",
    "\n",
    "d_relation = distillation_methods_module.Relations_Distiller(hint_layers=hint_layers, guided_layers=guided_layers, teacher=teacher, student=student, optimizer=optimizer)\n",
    "d_relation.train(train_iter, test_iter, num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.distenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d0557164c183bc22d7390206f2e8a2deecaeb7985853eddae99ef9c3a8b0152d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
