{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "vCXr6dNBWywr"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preamble"
      ],
      "metadata": {
        "id": "oCzLunkyW78w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### About\n",
        "This notebook provides an interface for experimenting with three knowledge distillation methods (Hinton et al.’s (2014), Romero et al.’s (2015) and Yim et al.’s (2017)) on CIFAR-100 (Krizhevsky, 2009). An account at Weights and Biases (https://www.wandb.ai) is required for logging results---your API key will be requested during initialisation.\n",
        "\n",
        "The implementations of each knowledge distillation method can be found at https://github.com/hnsyprst/DistillationComparison.\n",
        "\n",
        "### Instructions\n",
        "Set the paths in the 'Setup Save and Load Paths' cell (under 'Initialisation'). TEACHER_PATH should point to a ResNet50 model trained on CIFAR-100 (we provide a link to the model used in our experiments in the repository above). SAVE_PATH should point to the location where the student will be saved post-distillation. Change the parameters in the 'Launch a New Experiment' cell (under 'Conduct Experiments') to setup a new experiment. Restart the notebook and run all cells. The results will be logged in the specified project on Weights and Biases. \n",
        "\n",
        "### References:\n",
        "Krizhevsky, A. (2009) Learning Multiple Layers of Features from Tiny Images. University of Toronto.\n",
        "\n",
        "Liu, S., Johns, E. and Davison, A.J. (2019)\n",
        "‘End-to-End Multi-Task Learning with Attention’. arXiv. Available at: http://arxiv.org/abs/1803.10704 (Accessed: 31 October 2022).\n",
        "\n",
        "Omelchenko, I. (2020) pytorch - set seed everything, PyTorch Seed Everything. Available at: https://gist.github.com/ihoromi4/b681a9088f348942b01711f251e5f964 (Accessed: 20 November 2022).\n",
        "\n",
        "Romero, A., Ballas, N., Kahou, S.E., Chassang, A., Gatta, C. and Bengio, Y. (2015) \n",
        "‘FitNets: Hints for Thin Deep Nets’. arXiv. Available at: http://arxiv.org/abs/1412.6550 (Accessed: 31 August 2022).\n",
        "\n",
        "Yim, J., Joo, D., Bae, J. and Kim, J. (2017) ‘A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning’,\n",
        "in 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, HI: IEEE, pp. 7130–7138. Available at: https://doi.org/10.1109/CVPR.2017.754 (Accessed: 20 November 2022)."
      ],
      "metadata": {
        "id": "gH37bizu6UB2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialisation"
      ],
      "metadata": {
        "id": "sdlFJtFhWuXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Setup Save and Load Paths\n",
        "#@markdown Google Drive is mounted here\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "\n",
        "TEACHER_PATH = '/content/gdrive/MyDrive/TrainedModels/ResNets/ResNet50/4712.tar' #@param {type:\"string\"}\n",
        "SAVE_PATH = '/content/gdrive/MyDrive/TrainedModels/newDistillationExperiments/CIFAR100/' #@param {type:\"string\"}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "OFodHfqC_zPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qWZBcrdX5W-2"
      },
      "outputs": [],
      "source": [
        "#@title Download Repo and Login to Weights & Biases\n",
        "#@markdown \n",
        "\n",
        "!git clone https://github.com/hnsyprst/DistillationComparison.git\n",
        "!cp -a /content/DistillationComparison/. /content/\n",
        "!rm -r /content/DistillationComparison/\n",
        "\n",
        "!pip install wandb\n",
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import and Download Libraries\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import torch\n",
        "import torch.profiler\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.utils import data\n",
        "from torchvision import transforms\n",
        "from torchvision import models\n",
        "from torchvision.models.resnet import resnet101, resnet50, resnet34, resnet18\n",
        "\n",
        "!pip install torchmetrics\n",
        "import torchmetrics\n",
        "\n",
        "!pip install fvcore\n",
        "from fvcore.nn import FlopCountAnalysis\n",
        "\n",
        "!pip install ptflops\n",
        "from ptflops import get_model_complexity_info\n",
        "\n",
        "import training_utils as utils\n",
        "import network_utils as nutils\n",
        "\n",
        "import distillation_methods_module\n",
        "\n",
        "import copy\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import wandb\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(device)\n",
        "\n",
        "torch.autograd.set_detect_anomaly(False)\n",
        "torch.autograd.profiler.profile(False)\n",
        "torch.autograd.profiler.emit_nvtx(False)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XRCsa9IS5qKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download the Training and Test Sets\n",
        "mean = 0.5070751592371323, 0.48654887331495095, 0.4409178433670343\n",
        "std = 0.2673342858792401, 0.2564384629170883, 0.27615047132568404\n",
        "\n",
        "trans_train = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
        "                                  transforms.ToTensor(),\n",
        "                                  transforms.Normalize(mean, std)])\n",
        "trans_test = transforms.Compose([transforms.ToTensor(),\n",
        "                                 transforms.Normalize(mean, std)])\n",
        "\n",
        "mnist_train = torchvision.datasets.CIFAR100(\n",
        "    root=\"../data\", train=True, transform=trans_train, download=True)\n",
        "mnist_test = torchvision.datasets.CIFAR100(\n",
        "    root=\"../data\", train=False, transform=trans_test, download=True)\n",
        "\n",
        "batch_size = 512\n",
        "\n",
        "train_iter = data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "test_iter = data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "EpgauLx56HDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions"
      ],
      "metadata": {
        "id": "vCXr6dNBWywr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper Function For Seeding Relevant Random Number Generators\n",
        "#@markdown Code modified from Omelchenko's Gist (2020)\n",
        "\n",
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = \":4096:8\"\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "Ei7Qsk9z6O1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper Functions for Saving and Loading Models\n",
        "\n",
        "def load_model(model_type, pretrained=False, path=None):\n",
        "    num_classes = 100\n",
        "\n",
        "    model = copy.deepcopy(model_type)\n",
        "    num_ftrs = model.fc.in_features\n",
        "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "    optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
        "\n",
        "    if pretrained:\n",
        "        checkpoint = torch.load(path)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        epoch = checkpoint['epoch']\n",
        "        loss = checkpoint['loss']\n",
        "    \n",
        "    return model\n",
        "\n",
        "def save_model(model, optimizer, history, epoch, path):\n",
        "    torch.save({'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': history[1][-1]\n",
        "    }, path)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "MnxOGlkr63-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper Function for Getting Name from a Layer\n",
        "\n",
        "def get_layer_name(model, module, layer_index):\n",
        "    return nutils.get_name_from_layer(model, nutils.get_layer_in_module_from_index(module, layer_index))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "DTOGTFUy8F3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Functions to Tidy Relations and Features Distillation Settings\n",
        "\n",
        "def relations_hint_guided_layer_settings(teacher, student, teacher_output_layer_index, student_output_layer_index):\n",
        "    hint_layer_start_1 =    get_layer_name(teacher, teacher.layer1[0], teacher_output_layer_index)\n",
        "    hint_layer_end_1 =      get_layer_name(teacher, teacher.layer1[-1], teacher_output_layer_index)\n",
        "\n",
        "    hint_layer_start_2 =    get_layer_name(teacher, teacher.layer2[0], teacher_output_layer_index)\n",
        "    hint_layer_end_2 =      get_layer_name(teacher, teacher.layer2[-1], teacher_output_layer_index)\n",
        "\n",
        "    hint_layer_start_3 =    get_layer_name(teacher, teacher.layer3[0], teacher_output_layer_index)\n",
        "    hint_layer_end_3 =      get_layer_name(teacher, teacher.layer3[-1], teacher_output_layer_index)\n",
        "\n",
        "    hint_layers =           [(hint_layer_start_1, hint_layer_end_1), (hint_layer_start_2, hint_layer_end_2), (hint_layer_start_3, hint_layer_end_3)]\n",
        "\n",
        "\n",
        "    guided_layer_start_1 =  get_layer_name(student, student.layer1[0], student_output_layer_index)\n",
        "    guided_layer_end_1 =    get_layer_name(student, student.layer1[-1], student_output_layer_index)\n",
        "\n",
        "    guided_layer_start_2 =  get_layer_name(student, student.layer2[0], student_output_layer_index)\n",
        "    guided_layer_end_2 =    get_layer_name(student, student.layer2[-1], student_output_layer_index)\n",
        "\n",
        "    guided_layer_start_3 =  get_layer_name(student, student.layer3[0], student_output_layer_index)\n",
        "    guided_layer_end_3 =    get_layer_name(student, student.layer3[-1], student_output_layer_index)\n",
        "\n",
        "    guided_layers =         [(guided_layer_start_1, guided_layer_end_1), (guided_layer_start_2, guided_layer_end_2), (guided_layer_start_3, guided_layer_end_3)]\n",
        "\n",
        "    return hint_layers, guided_layers\n",
        "\n",
        "def features_hint_guided_layer_settings(teacher, student, teacher_output_layer_index, student_output_layer_index):\n",
        "    hint_layer = nutils.get_layer_in_module_from_index(teacher.layer3[-1], teacher_output_layer_index)\n",
        "    guided_layer = nutils.get_layer_in_module_from_index(student.layer3[-1], student_output_layer_index)\n",
        "\n",
        "    return hint_layer, guided_layer"
      ],
      "metadata": {
        "cellView": "form",
        "id": "lQcRqhGL7j8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper Function to Retrieve Model Complexity Information (MACs, parameter count)\n",
        "\n",
        "def get_model_info(model):\n",
        "    model.eval()\n",
        "    _, params = get_model_complexity_info(model, (3, 32, 32), as_strings=False, print_per_layer_stat=False, verbose=False)\n",
        "\n",
        "    input = torch.ones(1, 3, 32, 32).to(device)\n",
        "    mac_counter = FlopCountAnalysis(model, input)\n",
        "    macs = mac_counter.total()\n",
        "\n",
        "    return params, macs"
      ],
      "metadata": {
        "cellView": "form",
        "id": "dxwxrCBdAjyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Function for Launching a New Experiment\n",
        "\n",
        "def new_distillation_run(distiller_name, model_name, run_number, project_name, num_epochs, pretrained=False):\n",
        "    teacher_model_name = \"resnet50\"\n",
        "    teacher_model = models.resnet50(pretrained=False)\n",
        "\n",
        "    if model_name == \"resnet18\":\n",
        "        student_model = models.resnet18(pretrained=pretrained)\n",
        "    elif model_name == \"resnet34\":\n",
        "        student_model = models.resnet34(pretrained=pretrained)\n",
        "\n",
        "    if run_number == 0:\n",
        "        seed = 1059\n",
        "    elif run_number == 1:\n",
        "        seed = 3056\n",
        "    elif run_number == 2:\n",
        "        seed = 4967\n",
        "\n",
        "    seed_everything(seed)\n",
        "\n",
        "    student = load_model(student_model)\n",
        "    student.to(device)\n",
        "    teacher = load_model(teacher_model, pretrained=True, path=TEACHER_PATH)\n",
        "    teacher.to(device)\n",
        "    \n",
        "    lr = 1e-3\n",
        "    loss_fn = nn.CrossEntropyLoss(reduction='none').to(device)\n",
        "    optimizer = torch.optim.RMSprop(student.parameters(), lr=lr)\n",
        "    params, macs = get_model_info(student)\n",
        "\n",
        "    distiller = setup_distiller(teacher, student, optimizer)[distiller_name]\n",
        "\n",
        "    wandb.init(group=\"default-distillation\", project=project_name, config={\n",
        "              \"subgroup\": \"logits-bigtest\",\n",
        "              \"model\": model_name,\n",
        "              \"teacher\": teacher_model_name,\n",
        "              \"learning_rate\": lr,\n",
        "              \"epochs\": num_epochs,\n",
        "              \"batch_size\": batch_size,\n",
        "              \"seed\": seed,\n",
        "              \"macs\": macs,\n",
        "              \"params\": params})\n",
        "        \n",
        "    history = distiller.train(train_iter, test_iter, num_epochs, wandb_log=True)\n",
        "    save_model(student, optimizer, history, num_epochs-1, '{0}/{1}/{2}_{3}.tar'.format(SAVE_PATH, distiller_name, model_name, run_number))\n",
        "\n",
        "    top_1 = utils.evaluate_accuracy(student, test_iter, top_k=1)\n",
        "    top_5 = utils.evaluate_accuracy(student, test_iter, top_k=5)\n",
        "    print(top_1, top_5)\n",
        "    wandb.config.update({\"top_1\": top_1, \"top_5\": top_5})"
      ],
      "metadata": {
        "cellView": "form",
        "id": "f-GO2lf_7dyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Function for Seting Up Distillers\n",
        "\n",
        "def setup_distiller(teacher, student, optimizer):\n",
        "    hint_layers, guided_layers = relations_hint_guided_layer_settings(teacher, student, -1, -1)\n",
        "    hint_layer, guided_layer = features_hint_guided_layer_settings(teacher, student, -3, -2)\n",
        "\n",
        "    distiller_dict = {\"logits\":       distillation_methods_module.Logits_Distiller(temp=7, hard_loss_weight=0.2, teacher=teacher, student=student, optimizer=optimizer),\n",
        "                      \"features\":     distillation_methods_module.Features_Distiller(hint_layer=hint_layer, guided_layer=guided_layer, is_2D=True, temp=7, hard_loss_weight=0.05, teacher=teacher, student=student, optimizer=optimizer),\n",
        "                      \"relations\":    distillation_methods_module.Relations_Distiller(hint_layers=hint_layers, guided_layers=guided_layers, teacher=teacher, student=student, optimizer=optimizer),\n",
        "                      \"logits-DWA\":   distillation_methods_module.Logits_Distiller_DWA(temp=7, weight_temp=2, teacher=teacher, student=student, optimizer=optimizer),\n",
        "                      \"features-DWA\": distillation_methods_module.Features_Distiller_DWA(hint_layer=hint_layer, guided_layer=guided_layer, is_2D=True, temp=7, weight_temp=2, teacher=teacher, student=student, optimizer=optimizer)}\n",
        "\n",
        "    return distiller_dict"
      ],
      "metadata": {
        "cellView": "form",
        "id": "RCtwgq7uBMjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conduct Experiments"
      ],
      "metadata": {
        "id": "HZFuzpHBW3aL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Launch a New Experiment\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown #### Select a distillation method:\n",
        "distiller_name = \"logits\" #@param [\"logits\", \"features\", \"relations\", \"logits-DWA\", \"features-DWA\"] {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown #### Select a model architecture:\n",
        "model_name = \"resnet18\" #@param [\"resnet18\", \"resnet34\"] {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown #### Select the run number to determine the seed to use:\n",
        "run_number = 0 #@param [0, 1, 2] {type:\"raw\"}\n",
        "#@markdown ---\n",
        "#@markdown #### Enter the name of the project on wandb.me in which to log results:\n",
        "project_name = \"distillation-experiments\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown #### Enter the number of epochs:\n",
        "num_epochs = 25 #@param\n",
        "#@markdown ---\n",
        "#@markdown #### Choose whether students should be initialised with weights pretrained on ImageNet:\n",
        "pretrained = False #@param {type:\"boolean\"}\n",
        "\n",
        "new_distillation_run(distiller_name, model_name, run_number, project_name, num_epochs, pretrained=pretrained)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ZlopAVPLEdm2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}